{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj0IWnW4Ksui"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "University of North Carolina Charlotte<br>\n",
        "DSBA-6162 Data Mining<br>\n",
        "Instructor: Dr. Xi (Sunshine) Niu<br>\n",
        "<br>\n",
        "Nathan Schaaf<br>\n",
        "3/25/2025"
      ],
      "metadata": {
        "id": "hABc9hilPLKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA Topic Modeling Exercise\n",
        "In this exercise we apply LDA Topic Modeling on the entire corpus and extract k=10 topics. The example corpus contains 180 movie reviews, eact text file (review) written by one of two reviewers. The first 80 were written by Berardinelli and the remaining were by Schwartz."
      ],
      "metadata": {
        "id": "rtSr1KlhPR1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to work through this exercise, you will first need to clone the repository. Please refer to the README.MD file."
      ],
      "metadata": {
        "id": "yD8QS4pqPs8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpUef51xL65O",
        "outputId": "18127305-b49a-46b8-c975-d274563ae97d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Data_Mining/08"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UzJWKzSME7j",
        "outputId": "25343f8c-036e-4a41-b86f-2b2227048718"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Data_Mining/08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zchf4gGKxUu",
        "outputId": "f15eeabe-1c46-48d1-acd8-279a125ac050"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim (from -r requirements.txt (line 1))\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.8.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.14.1)\n",
            "Collecting numpy (from -r requirements.txt (line 3))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from -r requirements.txt (line 6))\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->-r requirements.txt (line 1)) (7.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 4)) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 4)) (2025.1.31)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->-r requirements.txt (line 1)) (1.17.2)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 4)) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 4)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 4)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 4)) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 4)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 4)) (0.1.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q_XrCeJgKsuk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n",
        "List the top 10 words for each topic."
      ],
      "metadata": {
        "id": "8mhnRc0-P000"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load Spacy model for text preprocessing\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moPEXS92N3gx",
        "outputId": "4c624a2e-06d4-4973-92ed-d95cc43f021f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to corpus\n",
        "corpus_path = \"/content/drive/My Drive/Data_Mining/08/MovieReviews\""
      ],
      "metadata": {
        "id": "_LYagPAkN545"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and preprocess the documents\n",
        "documents = []\n",
        "for file in os.listdir(corpus_path):\n",
        "    file_path = os.path.join(corpus_path, file)\n",
        "    if os.path.isfile(file_path):  # Ensure it's a file\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\", errors=\"replace\") as f:\n",
        "            text = f.read()\n",
        "            # Tokenization, lemmatization, stopword removal\n",
        "            doc = nlp(text.lower())\n",
        "            words = [token.lemma_ for token in doc if token.is_alpha and token.text not in stopwords.words(\"english\")]\n",
        "            documents.append(words)"
      ],
      "metadata": {
        "id": "uVwnHloMN9gJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary and corpus\n",
        "dictionary = corpora.Dictionary(documents)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "s4oPOHnCOAow"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LDA model\n",
        "num_topics = 10\n",
        "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)"
      ],
      "metadata": {
        "id": "18rOF_sNM1sq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print top 10 words for each topic\n",
        "topics = lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False)\n",
        "for i, topic in topics:\n",
        "    words = [word for word, _ in topic]\n",
        "    print(f\"Topic {i + 1}: {', '.join(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFGFFGW7OC55",
        "outputId": "d6df24a6-0bb1-404c-c876-0b1a7e00df6d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: film, man, one, take, go, war, movie, like, get, even\n",
            "Topic 2: film, family, leave, go, one, seem, joon, act, tell, like\n",
            "Topic 3: film, movie, one, make, story, go, character, get, life, lose\n",
            "Topic 4: film, make, get, ali, see, child, much, life, kid, problem\n",
            "Topic 5: film, one, see, make, get, take, story, life, go, even\n",
            "Topic 6: film, movie, one, make, like, get, good, story, character, go\n",
            "Topic 7: film, get, one, see, story, go, take, give, make, movie\n",
            "Topic 8: film, one, movie, get, make, go, see, life, story, well\n",
            "Topic 9: film, one, make, see, story, like, get, life, go, man\n",
            "Topic 10: film, one, movie, life, even, make, well, get, play, much\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2\n",
        "List the topic coverage probabilities for each document in this corpus."
      ],
      "metadata": {
        "id": "B_Qm24-TQHQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic distribution for each document\n",
        "topic_distributions = []\n",
        "for i, doc in enumerate(corpus):\n",
        "    topic_probs = lda_model.get_document_topics(doc, minimum_probability=0)\n",
        "    topic_probs_dict = {f\"Topic_{topic_id}\": prob for topic_id, prob in topic_probs}\n",
        "    topic_probs_dict[\"Document\"] = f\"Doc_{i+1}\"\n",
        "    topic_distributions.append(topic_probs_dict)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "df_topic_dist = pd.DataFrame(topic_distributions)\n",
        "df_topic_dist.set_index(\"Document\", inplace=True)"
      ],
      "metadata": {
        "id": "ZzAo-bFEQbHT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display topic probabilities for each document\n",
        "print(df_topic_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuhmd-sNQb_S",
        "outputId": "6dbbc058-30d0-4b54-839c-c60d247294a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Topic_0   Topic_1   Topic_2   Topic_3   Topic_4   Topic_5  \\\n",
            "Document                                                               \n",
            "Doc_1     0.000312  0.000312  0.000312  0.000312  0.000312  0.000312   \n",
            "Doc_2     0.000266  0.000266  0.000266  0.000266  0.997610  0.000266   \n",
            "Doc_3     0.998476  0.000169  0.000169  0.000169  0.000169  0.000169   \n",
            "Doc_4     0.000143  0.000143  0.000143  0.000143  0.998713  0.000143   \n",
            "Doc_5     0.000236  0.000236  0.000236  0.000236  0.000236  0.997879   \n",
            "...            ...       ...       ...       ...       ...       ...   \n",
            "Doc_176   0.000256  0.000256  0.000256  0.000256  0.000256  0.408640   \n",
            "Doc_177   0.000197  0.000197  0.000197  0.000197  0.503617  0.494809   \n",
            "Doc_178   0.000217  0.000217  0.000217  0.000217  0.000217  0.007227   \n",
            "Doc_179   0.000208  0.000208  0.000208  0.000208  0.000208  0.000208   \n",
            "Doc_180   0.000314  0.000314  0.000314  0.000314  0.000314  0.144565   \n",
            "\n",
            "           Topic_6   Topic_7   Topic_8   Topic_9  \n",
            "Document                                          \n",
            "Doc_1     0.000312  0.000312  0.997194  0.000312  \n",
            "Doc_2     0.000266  0.000266  0.000266  0.000266  \n",
            "Doc_3     0.000169  0.000169  0.000169  0.000169  \n",
            "Doc_4     0.000143  0.000143  0.000143  0.000143  \n",
            "Doc_5     0.000236  0.000236  0.000236  0.000236  \n",
            "...            ...       ...       ...       ...  \n",
            "Doc_176   0.000256  0.589311  0.000256  0.000256  \n",
            "Doc_177   0.000197  0.000197  0.000197  0.000197  \n",
            "Doc_178   0.000217  0.000217  0.991040  0.000217  \n",
            "Doc_179   0.000208  0.000208  0.998128  0.000208  \n",
            "Doc_180   0.000314  0.852925  0.000314  0.000314  \n",
            "\n",
            "[180 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "## 1. Top 10 Words for Each Topic\n",
        "\n",
        "The top 10 words identified for each topic suggest that the model is detecting specific patterns within the movie reviews, predominantly centered around general themes in film, character development, and emotions associated with film watching.\n",
        "\n",
        "- **Topic 1**: Focuses on general movie-related terms such as *film, man, movie, war*, and *get*, which may reflect action or war films.\n",
        "- **Topic 2**: Shows terms related to family dynamics (*family, leave, act, joon*), possibly indicating themes of family drama or relationships.\n",
        "- **Topic 3**: Emphasizes emotional or plot elements, such as *story, character, life*, which could signify narratives focused on personal growth or struggle.\n",
        "- **Topic 4**: Seems to center around children's films or family-friendly genres, evident from words like *child, kid, problem, life*.\n",
        "- **Topic 5**: Again shows a focus on storytelling, with *story, life, make, even*, highlighting narrative-driven films.\n",
        "- **Topic 6**: Suggests a focus on quality and story, with words like *good, story, character, movie* pointing to well-developed films.\n",
        "- **Topic 7**: Includes terms such as *give, make, movie, story*, possibly reflecting themes of contribution or action within film narratives.\n",
        "- **Topic 8**: Indicates a focus on films with a life-lesson or personal journey aspect, evident from words like *life, story, make, well*.\n",
        "- **Topic 9**: Appears to focus on dramatic elements, with terms like *life, movie, story*, reflecting more intense or emotional narratives.\n",
        "- **Topic 10**: Combines general movie-related terms with terms like *play* and *much*, which might suggest films involving performance or highly engaging content.\n",
        "\n",
        "## 2. Topic Coverage Probabilities for Each Document\n",
        "\n",
        "The topic coverage probabilities show how each document (movie review) is associated with each of the 10 topics. The distribution of probabilities indicates that certain documents are highly associated with specific topics, with one topic often being dominant per document. For instance:\n",
        "\n",
        "- **Document 1** is mainly associated with Topic 8, with a probability of 0.997194.\n",
        "- **Document 2** shows a strong association with Topic 4, with a probability of 0.997610.\n",
        "- **Document 3** has a strong association with Topic 0, with a probability of 0.998476.\n",
        "- **Document 4** has a high probability (0.998713) for Topic 4.\n",
        "- **Document 180** is most strongly associated with Topic 7, with a probability of 0.852925.\n",
        "\n",
        "Overall, the topic modeling results provide a clear indication of the themes present in the corpus of movie reviews, with each document showing a varying degree of association with the topics generated by the LDA model.\n"
      ],
      "metadata": {
        "id": "-F8RU6toRI6O"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}