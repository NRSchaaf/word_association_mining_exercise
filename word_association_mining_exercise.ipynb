{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSBA-6162<br>\n",
    "Nathan Schaaf<br>\n",
    "3/18/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Association Mining Exercise\n",
    "In this exercise we will explore some basic word association mining techniques. The example corpus contains 180 movie reviews, eact text file (review) written by one of two reviewers. The first 80 were written by Berardinelli and the remaining were by Schwartz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to work through this exercise, you will first need to clone the repository. Please refer to the README.MD file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import Counter # Used in Problem 3\n",
    "import nltk # Used in Problem 3\n",
    "from nltk.corpus import stopwords # Used in Problem 3\n",
    "import string # Used in Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Calculate the entropy of the word \"director\" appearance in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing the movie reviews\n",
    "folder_path = r\"C:/Users/natha/OneDrive/Desktop/Data_Mining/07/word_association_mining_exercise/MovieReviews\"  # Replace this path this with your location of the cloned repo\n",
    "\n",
    "# Word to analyze\n",
    "word = \"director\"\n",
    "\n",
    "# List to store word counts per file\n",
    "word_counts = []\n",
    "\n",
    "# Iterate through each text file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):  # Ensure we only process text files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:  # Use encoding='latin-1 as 'utf-8' has errors\n",
    "            text = file.read().lower()  # Convert to lowercase for consistency\n",
    "            count = text.split().count(word)  # Count occurrences of the word\n",
    "            word_counts.append(count)\n",
    "\n",
    "# Compute probability distribution\n",
    "total_occurrences = sum(word_counts)\n",
    "if total_occurrences == 0:\n",
    "    print(f\"The word '{word}' does not appear in the corpus.\")\n",
    "    exit(1)\n",
    "\n",
    "probabilities = [count / total_occurrences for count in word_counts]\n",
    "\n",
    "# Calculate Shannon entropy\n",
    "entropy = -sum(p * math.log2(p) for p in probabilities if p > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the word 'director' in the corpus is 5.4528 bits.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The entropy of the word '{word}' in the corpus is {entropy:.4f} bits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "<p><strong>Entropy</strong> is a measure of uncertainty or randomness in a distribution. In this case, the distribution of the word \"director\" across the 180 movie reviews.</p>\n",
    "<p>Entropy is measured in bits (binary digits). It represents the smallest possible amount of uncertainty, e.g., a single yes/no or 0/1 decision.</p>\n",
    "<p>A <strong>low entropy</strong> is closer to zero and indicates a high predicatability.<br>\n",
    "A <strong>high entropy</strong>, in this case 5.45 bits, means the appearance of the word \"director\" is more unpredicatable across the corpus of reviews.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Calculate the mutual informatino between the \"director\" and the document author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reviews per author\n",
    "num_reviews_B = 80\n",
    "num_reviews_S = 100\n",
    "total_reviews = num_reviews_B + num_reviews_S\n",
    "\n",
    "# Initialize counters\n",
    "c_BY = c_BN = c_SY = c_SN = 0\n",
    "\n",
    "# Iterate through the first 80 reviews (Berardinelli)\n",
    "for i, filename in enumerate(sorted(os.listdir(folder_path))):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='latin-1', errors='ignore') as file:\n",
    "            text = file.read().lower()\n",
    "            if word in text:\n",
    "                if i < 80:  # First 80 files → Berardinelli\n",
    "                    c_BY += 1\n",
    "                else:       # Remaining files → Schwartz\n",
    "                    c_SY += 1\n",
    "\n",
    "# Compute missing values\n",
    "c_BN = num_reviews_B - c_BY\n",
    "c_SN = num_reviews_S - c_SY\n",
    "\n",
    "# Compute probabilities\n",
    "P_B = num_reviews_B / total_reviews\n",
    "P_S = num_reviews_S / total_reviews\n",
    "P_Y = (c_BY + c_SY) / total_reviews\n",
    "P_N = (c_BN + c_SN) / total_reviews\n",
    "\n",
    "P_BY = c_BY / total_reviews\n",
    "P_BN = c_BN / total_reviews\n",
    "P_SY = c_SY / total_reviews\n",
    "P_SN = c_SN / total_reviews\n",
    "\n",
    "# Compute Mutual Information\n",
    "def safe_log2(x):\n",
    "    return math.log2(x) if x > 0 else 0  # Avoid log(0)\n",
    "\n",
    "MI = (\n",
    "    P_BY * safe_log2(P_BY / (P_B * P_Y)) +\n",
    "    P_BN * safe_log2(P_BN / (P_B * P_N)) +\n",
    "    P_SY * safe_log2(P_SY / (P_S * P_Y)) +\n",
    "    P_SN * safe_log2(P_SN / (P_S * P_N))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between 'director' and the document author: 0.0541 bits\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mutual Information between 'director' and the document author: {MI:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "<p><strong>Mutual Information (MI)</strong> quantifies the reduction in uncertainty about one variable (in this case the author) given knowledge of another variable (in this case the appearance of the word \"director\"). In other words, MI can tell us how much knowing the document author helps to predict the presence of the word \"director\" in a review.</p>\n",
    "<p>A <strong>high MI value</strong> indicates knowing the author give a lot of information about whether \"director\" appears.<br>A <strong>low MI value</strong> indicates the presence of \"director\" is not strongly linked to the author.</p>\n",
    "<p>Steps to compute the MI:\n",
    "    <ol>\n",
    "        <li>Iterate through the reviews and label which reviews are Berardinelli vs. Schwartz.</li>\n",
    "        <li>Compute the missing values, where a review *does not* contain the word \"director\".</li>\n",
    "        <li>Compute the probabilities (Yes/No) for each author and if the word \"director\" appears.</li>\n",
    "        <li>Compute the mutual informaitn using the MI formula.</li>\n",
    "    </ol>\n",
    "</p>\n",
    "<p>In this exercise, the MI of 0.0541 bits means that knowing whether the word \"director\" appears in a review provides a small amount of informatino about who wrote teh review, either Berardinelli or Schwartz.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Find the <strong>Top Ten</strong> words with the highest mutual information with the document autor and their respective mutual information. Filter out the 'stop words'.\n",
    "<p><strong>Stop Words</strong> are common words like \"the\", \"a\", \"is\", etc. that are often removed from text before analysis because they do not carry much meaning on their own. For this exercise, we can use the Python library NLTK (Natural Language Toolkit) to gain access to a pre-defined list of stop words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stop words from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load NLTK's stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all reviews and process them\n",
    "def load_reviews(folder_path):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text = file.read().lower()\n",
    "                reviews.append(text)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the reviews (tokenize, clean, remove punctuation, and remove stop words)\n",
    "def preprocess_reviews(reviews):\n",
    "    word_counts = Counter()\n",
    "    total_words = 0\n",
    "    for review in reviews:\n",
    "        # Remove punctuation\n",
    "        review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = review.split()\n",
    "        # Remove stop words\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        total_words += len(filtered_words)\n",
    "        word_counts.update(filtered_words)\n",
    "    return word_counts, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words for each author (Berardinelli and Schwartz)\n",
    "def count_words_for_authors(reviews):\n",
    "    author_reviews = {'Berardinelli': reviews[:80], 'Schwartz': reviews[80:]}\n",
    "    author_word_counts = {'Berardinelli': Counter(), 'Schwartz': Counter()}\n",
    "    total_words_by_author = {'Berardinelli': 0, 'Schwartz': 0}\n",
    "\n",
    "    for author, author_reviews_list in author_reviews.items():\n",
    "        for review in author_reviews_list:\n",
    "            # Remove punctuation\n",
    "            review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "            words = review.split()\n",
    "            # Remove stop words\n",
    "            filtered_words = [word for word in words if word not in stop_words]\n",
    "            total_words_by_author[author] += len(filtered_words)\n",
    "            author_word_counts[author].update(filtered_words)\n",
    "\n",
    "    return author_word_counts, total_words_by_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mutual information for each word\n",
    "def calculate_mutual_information(word_counts, author_word_counts, total_words_by_author, total_reviews, author_totals):\n",
    "    mutual_info = {}\n",
    "    for word in word_counts:\n",
    "        P_w = word_counts[word] / sum(word_counts.values())  # Probability of the word in the corpus\n",
    "        for author in ['Berardinelli', 'Schwartz']:\n",
    "            P_A = author_totals[author] / total_reviews  # Probability of the author\n",
    "            P_w_A = author_word_counts[author].get(word, 0) / total_words_by_author[author]  # Probability of word given author\n",
    "            if P_w_A > 0:  # To avoid log(0)\n",
    "                MI = P_w_A * math.log2(P_w_A / (P_w * P_A))\n",
    "                mutual_info[word] = mutual_info.get(word, 0) + MI\n",
    "    return mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews and process\n",
    "reviews = load_reviews(folder_path)\n",
    "word_counts, total_words = preprocess_reviews(reviews)\n",
    "\n",
    "# Calculate word counts for each author\n",
    "author_word_counts, total_words_by_author = count_words_for_authors(reviews)\n",
    "author_totals = {'Berardinelli': 80, 'Schwartz': 100}\n",
    "total_reviews_count = 180  # The total number of reviews (80 from Berardinelli, 100 from Schwartz)\n",
    "\n",
    "# Calculate mutual information for each word\n",
    "mutual_info = calculate_mutual_information(word_counts, author_word_counts, total_words_by_author, total_reviews_count, author_totals)\n",
    "\n",
    "# Get top 10 words with the highest mutual information\n",
    "top_10_words = sorted(mutual_info.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 'film', MI: 0.0323 bits\n",
      "Word: 'one', MI: 0.0156 bits\n",
      "Word: 'movie', MI: 0.0111 bits\n",
      "Word: 'story', MI: 0.0107 bits\n",
      "Word: 'even', MI: 0.0089 bits\n",
      "Word: 'life', MI: 0.0077 bits\n",
      "Word: 'like', MI: 0.0075 bits\n",
      "Word: 'see', MI: 0.0075 bits\n",
      "Word: 'schwartz', MI: 0.0069 bits\n",
      "Word: 'dennis', MI: 0.0068 bits\n"
     ]
    }
   ],
   "source": [
    "# Display the top 10 words and their MI values\n",
    "for word, mi_value in top_10_words:\n",
    "    print(f\"Word: '{word}', MI: {mi_value:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "<p>We can follow these steps calculate the top ten words with the highest mutual information (MI) with the document author.</p>\n",
    "\n",
    "<p>Steps:\n",
    "    <ol>\n",
    "        <li><strong>Preprocess the text</strong>:\n",
    "            <ul>\n",
    "                <li>Read the reviews and clean the data (i.e., lowercase, remove punctuation, etc.).</li>\n",
    "                <li>Remove <strong>stop words</strong>.</li>\n",
    "                <li>Tokenize the reviews into individual words.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Count occurances of each word for each reviewer</strong>.</li>\n",
    "        <li><strong>Compute mutual information (MI) for each word</strong>:\n",
    "            <ul>\n",
    "                <li>Calculate how often each word appears in each author's reviews.</li>\n",
    "                <li>Use the MI formula between the word and document author.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Sort words by their MI score and select top 10</strong>.</li>\n",
    "    </ol>\n",
    "</p>\n",
    "<p>The <strong>Top Ten</strong> words sorted by highest MI are:\n",
    "    <ol>\n",
    "    <li>film</li>\n",
    "    <li>one</li>\n",
    "    <li>movie</li>\n",
    "    <li>story</li>\n",
    "    <li>even</li>\n",
    "    <li>life</li>\n",
    "    <li>like</li>\n",
    "    <li>see</li>\n",
    "    <li>schwartz</li>\n",
    "    <li>dennis</li>\n",
    "    </ol>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
